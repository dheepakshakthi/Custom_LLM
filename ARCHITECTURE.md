# LLM-200M Architecture Diagram

```
┌─────────────────────────────────────────────────────────────────────────┐
│                          LLM-200M Architecture                           │
│                        (200 Million Parameters)                          │
└─────────────────────────────────────────────────────────────────────────┘

                              INPUT TEXT
                                  │
                                  ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                         TOKENIZATION (GPT-2)                             │
│  "Hello world" → [15496, 995] → [vocab_size=50257, max_len=2048]       │
└─────────────────────────────────────────────────────────────────────────┘
                                  │
                                  ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                      TOKEN EMBEDDING LAYER                               │
│  Input:  [batch, seq_len] = [B, L]                                     │
│  Output: [batch, seq_len, embed_dim] = [B, L, 768]                     │
│  Params: 50257 × 768 = 38,597,376                                      │
└─────────────────────────────────────────────────────────────────────────┘
                                  │
                                  ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                          DROPOUT (p=0.1)                                 │
└─────────────────────────────────────────────────────────────────────────┘
                                  │
                                  ▼
        ┌─────────────────────────────────────────────────┐
        │         24 × TRANSFORMER BLOCKS                 │
        │                                                 │
        │  Each block contains:                           │
        └─────────────────────────────────────────────────┘
                                  │
                    ┌─────────────┴─────────────┐
                    ▼                           ▼
        ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
        ┃          TRANSFORMER BLOCK (×24)                ┃
        ┃                                                  ┃
        ┃  ┌────────────────────────────────────────┐    ┃
        ┃  │  1. PRE-LAYER NORMALIZATION            │    ┃
        ┃  │     Input: [B, L, 768]                 │    ┃
        ┃  │     Output: [B, L, 768]                │    ┃
        ┃  └────────────────────────────────────────┘    ┃
        ┃                    │                            ┃
        ┃                    ▼                            ┃
        ┃  ┌────────────────────────────────────────┐    ┃
        ┃  │  2. MULTI-HEAD ATTENTION (12 heads)    │    ┃
        ┃  │                                         │    ┃
        ┃  │  ┌──────────────────────────────────┐  │    ┃
        ┃  │  │  QKV Projection                  │  │    ┃
        ┃  │  │  In:  [B, L, 768]               │  │    ┃
        ┃  │  │  Out: 3 × [B, L, 768]           │  │    ┃
        ┃  │  └──────────────────────────────────┘  │    ┃
        ┃  │                │                        │    ┃
        ┃  │                ▼                        │    ┃
        ┃  │  ┌──────────────────────────────────┐  │    ┃
        ┃  │  │  Reshape to Multi-Head           │  │    ┃
        ┃  │  │  [B, 12, L, 64] (head_dim=64)   │  │    ┃
        ┃  │  └──────────────────────────────────┘  │    ┃
        ┃  │                │                        │    ┃
        ┃  │                ▼                        │    ┃
        ┃  │  ┌──────────────────────────────────┐  │    ┃
        ┃  │  │  Rotary Position Embedding       │  │    ┃
        ┃  │  │  (RoPE - better than absolute)   │  │    ┃
        ┃  │  │  Applies to Q and K              │  │    ┃
        ┃  │  └──────────────────────────────────┘  │    ┃
        ┃  │                │                        │    ┃
        ┃  │                ▼                        │    ┃
        ┃  │  ┌──────────────────────────────────┐  │    ┃
        ┃  │  │  Flash Attention (if available)  │  │    ┃
        ┃  │  │  OR Standard Attention           │  │    ┃
        ┃  │  │  QK^T / √d_k → softmax → *V     │  │    ┃
        ┃  │  │  Causal masking applied          │  │    ┃
        ┃  │  └──────────────────────────────────┘  │    ┃
        ┃  │                │                        │    ┃
        ┃  │                ▼                        │    ┃
        ┃  │  ┌──────────────────────────────────┐  │    ┃
        ┃  │  │  Output Projection               │  │    ┃
        ┃  │  │  [B, 12, L, 64] → [B, L, 768]   │  │    ┃
        ┃  │  └──────────────────────────────────┘  │    ┃
        ┃  └────────────────────────────────────────┘    ┃
        ┃                    │                            ┃
        ┃                    ▼                            ┃
        ┃  ┌────────────────────────────────────────┐    ┃
        ┃  │  3. RESIDUAL CONNECTION                │    ┃
        ┃  │     x = x + attention(x)               │    ┃
        ┃  └────────────────────────────────────────┘    ┃
        ┃                    │                            ┃
        ┃                    ▼                            ┃
        ┃  ┌────────────────────────────────────────┐    ┃
        ┃  │  4. PRE-LAYER NORMALIZATION            │    ┃
        ┃  └────────────────────────────────────────┘    ┃
        ┃                    │                            ┃
        ┃                    ▼                            ┃
        ┃  ┌────────────────────────────────────────┐    ┃
        ┃  │  5. FEED-FORWARD NETWORK (SwiGLU)      │    ┃
        ┃  │                                         │    ┃
        ┃  │  ┌──────────────────────────────────┐  │    ┃
        ┃  │  │  W1: [768 → 3072]               │  │    ┃
        ┃  │  │  W3: [768 → 3072]               │  │    ┃
        ┃  │  │  SwiGLU: SiLU(W1*x) ⊙ W3*x      │  │    ┃
        ┃  │  │  (⊙ = element-wise multiply)     │  │    ┃
        ┃  │  └──────────────────────────────────┘  │    ┃
        ┃  │                │                        │    ┃
        ┃  │                ▼                        │    ┃
        ┃  │  ┌──────────────────────────────────┐  │    ┃
        ┃  │  │  W2: [3072 → 768]               │  │    ┃
        ┃  │  └──────────────────────────────────┘  │    ┃
        ┃  │                │                        │    ┃
        ┃  │                ▼                        │    ┃
        ┃  │  ┌──────────────────────────────────┐  │    ┃
        ┃  │  │  Dropout (p=0.1)                │  │    ┃
        ┃  │  └──────────────────────────────────┘  │    ┃
        ┃  └────────────────────────────────────────┘    ┃
        ┃                    │                            ┃
        ┃                    ▼                            ┃
        ┃  ┌────────────────────────────────────────┐    ┃
        ┃  │  6. RESIDUAL CONNECTION                │    ┃
        ┃  │     x = x + ffn(x)                     │    ┃
        ┃  └────────────────────────────────────────┘    ┃
        ┃                                                  ┃
        ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
                                  │
                     (Repeat 24 times)
                                  │
                                  ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                      FINAL LAYER NORMALIZATION                           │
│  Input:  [B, L, 768]                                                    │
│  Output: [B, L, 768]                                                    │
└─────────────────────────────────────────────────────────────────────────┘
                                  │
                                  ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                         LM HEAD (Language Model Head)                    │
│  Linear: [768 → 50257] (tied with token embedding)                     │
│  Input:  [B, L, 768]                                                    │
│  Output: [B, L, 50257] (logits over vocabulary)                        │
└─────────────────────────────────────────────────────────────────────────┘
                                  │
                                  ▼
                            SOFTMAX (inference)
                                  │
                                  ▼
                        SAMPLING / ARGMAX
                                  │
                                  ▼
                          OUTPUT TOKENS
                                  │
                                  ▼
                          DETOKENIZATION
                                  │
                                  ▼
                            OUTPUT TEXT


┌─────────────────────────────────────────────────────────────────────────┐
│                         PARAMETER BREAKDOWN                              │
├─────────────────────────────────────────────────────────────────────────┤
│  Token Embedding:          50257 × 768 = 38,597,376                    │
│  24 Transformer Blocks:                                                 │
│    - Attention (per block):  ~2.4M × 24 = ~57.6M                       │
│    - FFN (per block):        ~7.1M × 24 = ~170.4M                      │
│    - Layer Norms:            negligible                                 │
│  Final Layer Norm:           1,536                                      │
│  LM Head:                    TIED (shares with embedding)               │
├─────────────────────────────────────────────────────────────────────────┤
│  TOTAL PARAMETERS:           ~251,000,000                               │
└─────────────────────────────────────────────────────────────────────────┘


┌─────────────────────────────────────────────────────────────────────────┐
│                         KEY DESIGN FEATURES                              │
├─────────────────────────────────────────────────────────────────────────┤
│  ✓ RoPE (Rotary Position Embedding)                                    │
│    - Better position encoding than absolute/learned                     │
│    - Enables length extrapolation                                       │
│                                                                          │
│  ✓ SwiGLU Activation                                                    │
│    - Superior to GELU and ReLU                                          │
│    - Used in modern LLMs (LLaMA, PaLM)                                 │
│                                                                          │
│  ✓ Pre-Normalization                                                    │
│    - More stable training for deep networks                             │
│    - Better gradient flow                                               │
│                                                                          │
│  ✓ Flash Attention (when available)                                     │
│    - 2-4× faster than standard attention                                │
│    - Reduced memory usage                                               │
│    - Exact attention (not approximate)                                  │
│                                                                          │
│  ✓ Weight Tying                                                         │
│    - Token embedding tied with LM head                                  │
│    - Reduces parameters, maintains performance                          │
│                                                                          │
│  ✓ Multi-Head Attention                                                 │
│    - 12 heads × 64 dim = 768 total                                     │
│    - Captures different aspects of relationships                        │
└─────────────────────────────────────────────────────────────────────────┘


┌─────────────────────────────────────────────────────────────────────────┐
│                         TRAINING PIPELINE                                │
└─────────────────────────────────────────────────────────────────────────┘

    PHASE 1: PRE-TRAINING              PHASE 2: FINE-TUNING
    ┌──────────────────┐               ┌──────────────────┐
    │   BookCorpus     │               │  OpenCoder-LLM   │
    │  (General Text)  │               │  (Instructions)  │
    └────────┬─────────┘               └────────┬─────────┘
             │                                  │
             ▼                                  ▼
    ┌──────────────────┐               ┌──────────────────┐
    │   Tokenization   │               │   Tokenization   │
    │   Max Len: 512   │               │   Max Len: 1024  │
    └────────┬─────────┘               └────────┬─────────┘
             │                                  │
             ▼                                  ▼
    ┌──────────────────┐               ┌──────────────────┐
    │  3 Epochs        │               │  2 Epochs        │
    │  LR: 3e-4        │               │  LR: 1e-4        │
    │  Batch: 32       │               │  Batch: 32       │
    │  Warmup: 2000    │               │  Warmup: 500     │
    └────────┬─────────┘               └────────┬─────────┘
             │                                  │
             ▼                                  ▼
    ┌──────────────────┐               ┌──────────────────┐
    │ Next-token pred  │               │ Instruction +    │
    │  Cross-entropy   │               │  Response        │
    └────────┬─────────┘               └────────┬─────────┘
             │                                  │
             └──────────┬───────────────────────┘
                        ▼
                 ┌─────────────┐
                 │ Final Model │
                 │   200M LLM  │
                 └─────────────┘
```

**Legend:**
- `┌─┐` = Rectangular boxes (operations)
- `┏━┓` = Thick boxes (major components)
- `│` = Vertical flow
- `▼` = Flow direction
- `⊙` = Element-wise multiplication
- `×` = Matrix multiplication or repetition
